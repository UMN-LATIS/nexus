<html>
<head>
 <link href="style.css" rel="stylesheet" type="text/css">
</head>

<body>
<h1>Nexus</h1>
<h2>A multiresolution mesh visualization library (in C++)</h1>

<h3>Introduction</h3>
<p>Nexus is a multiresolution, visualization library for very large surface models.
It belongs to the family of cluster based, view-dependent visualization algorithms described in BDAM, Adaptive Tetrapuzzles and Batched Multi Triangulation (BMT) but adopts a different spatial partitioning strategy based on KD-trees which combines fast streaming construction of the multiresolution model with adaptive spatial partitioning of the initial mesh.</p>

<p>While previous techniques only supported color per vertex information, the Nexus data format 
supports also multiresolution projective textures.

<p>Features of the library includes: support for huge models (tested up to 1 billion triangles), 
robustness to non-manifold and very irregularly sampled meshes, HTTP streaming, multiplatform code (Windows, Linux, Ma, Ios, WebGL), 
open source licence.</p>


<h2>Related Work</h2>

<p>Small intro on multiresolution early works, progressive meshes and the DAG, and the GPU > CPU problem</p>

<p>Many techniques have been proposed to reduce the workload of the CPU and generate GPU
optimized geometry by adapting previous algorithms:  mostly by amortizing the cost of the
extraction over several frames [67, 30, 52], or by caching geometry on the GPU as
vertex arrays and rendering them as triangle strips [89, 65].<p>

<p>TODO search literature for recent works.</p>

<h3>Cluster Based Multiresolution</h3>
<p>A different class of methods (BDAM, tetra, BMT) employs a patch-based approach: the granularity of the primitive
is moved from triangles to small contiguous portions of a mesh to reduce the number of per-triangle CPU operations. 
A batched structure allows for aggressive GPU optimization of the
triangle patches: local coherence can be exploited and triangle strips can be easily used; this
results in a huge boost in terms of performance of the graphic card: the increase in rendering rate
(one order of magnitude of triangles per second at least) largely compensates for the suboptimal
extraction of the mesh. Larger updates might also result in more evident popping effects, however,
the very small average dimension of the triangles, due to the much higher resolution attainable,
greatly reduces the effect.</p>

<p>The framework for this class of algorithm is described in BMT as a special case of MT (cite paper) where the primitive
is a small portion of the mesh and the DAG is generated by a sequence of partitioning of the model and their intersections. (see figure).</p>
<p>Each node in a partition correspond to a vertex in the DAG (see figure), the intersections between two consecutive partitions
correspond to an edge in the graph and a small patch of triangles. The patches associated to the edges entering a node form the triangles in the node after the simplification, those exiting the node form the triangles before the simplification.</p>

<p>A cut in the DAG is a subset of nodes where all parents of a node are in the cut. The patches corresponding to the edges exiting a cut join to create a representation of the whole model. Changing the cut in the DAG we can obtain models with different resolutions.</p>

<p>The main difference between Tetrapuzzles and BMT lies in the spatial partitioning algorithm: Tetrapuzzles relies
on a regular hierarchy of diamonds, while BMT builds an adaptive voronoi partition. 
The flexibility of the adaptive voronoi partition comes at the cost
of increased complexity and much longer multiresolution model preprocessing time.</p>

<p>We propose a different spatial partition based on KD-trees. This combines the fast streaming processing
of the mesh with adaptive spatial partitioning.</p>

<h2>Construction</h2>
<p>As described in BMT, a clustered multiresolution model can be generated by a hierarchy of spatial partitioning of the initial mesh, provided
each consecutive level share no common border. The diamond hierarchy and the voronoi partitioning are easily shown to respect this condition.</p>
<p>We propose a spatial partitioning based on the leaves of a sequence of KD-trees, where at each level we change the orientation of the axis, or equivalently we switch the splitting ratio betweeen 1/3 and 2/3 instead of the canonical 1/2. Both these solution ensures that each consecutive level partitioning is independent. (actually the alternating splitting produces sligthly less fragments, which means less openGL calls)</p>


<h3>Streaming</h3>
<p>In order to process meshes which could not fit in main memory, we adopt a streaming strategy:
the model is converted from the usual indexed representation to a triangle soup. During this stage the bounding box is computed.</p>
<p>Finding the splitting axis for a KD-tree would require processing the whole mesh leading to a log2(N) passes, we could quickly find a good approximation if, for each node, the beginning of the stream was a reasonably good sampling of the whole model. Since nodes down in the hierarchy will be split only when most of the triangle have been processed this requirement is equivalent to: all intervals starting at the beginning of the stream are
a good sampling of the whol model.</p>
<p>We can easily and efficiently achieve this by shuffling the faces while building the triangle soup; a completely random shuffling, however, would cripple our cache coherence and result in extreme swapping to disk of the main memory. The trick we used is to divide the position of a triangle by 2^n 
with probability 1/2^n. The easiest way to do this is to create log2(N) streams and append faces to the nth stream with probability 1/2^n.</p>

<h3>Building levels</h3>

<p>Construction of the multiresolution models is performed in a number steps, in each the number of triangles is halved:</p>
<ul>
<li>1: the triangles from the triangle soup stream are inserted into a KD-tree
<li>2: for each leaf of the tree we collect the triangles, generate a small mesh, which is saved as a node in the multiresolution model. The node needs to be split into patches according to the intersection with the previous partition, we can recover this information by storing in each triangle in which node it was contained.
<li>3: the borders of the mesh (the triangles whose vertices do not fully belong to the node) are marked as read-only and the mesh is simplified using a quadric simplification algorithm (actually any simplification algorithm could be used as long as the borders are preserved).
<li>4: finally the triangles of the simplified mesh are pushed (and shuffled) into a new triangle soup stream, and we proceed with step 1 again.
</al>

<p>The most expensive step by far is the simplification step. Notice how this can be easily performed in parallel as each node is independent from all the others.</p>

<h3>Optimization</h3>
<p>Each patch is stored on disk as a locally indexed mesh. A number of optimizations can be applied
to improve the rendering preformances and/or reduce its size on disk:</p>

<ul>
<li>Cache coherence reordering of the vertices increases rendering performances by minimizing vertex cache miss rate.
<li>Triangle strips slightly reduces size on disk and further improve vertex cache coherence
<li>Vertex and normal quantization with entropy encoding can significantly reduce size on disk albeit at a cost in decompression time.
</ul>

<h2>Rendering</h2>

<p>In order to separate the rendering thread from loading and unloading of the node data (from disk (or network) to the GPU) we use a multilevel cache on a different thread. (see picture).
The cache has been designed to support very frequent priority changes and to minimize interaction (and thus locking mechanism) with the rendering thread.
In particular each patch is locked and unlocked using atomic_integers, lazy priority updates and double headed heaps minimize sorting of the elements in the cache. Details can be found at http://vcg.isti.cnr.it/gcache</p>

<h3>Traversal</h3>

<p>Rendering a Nexus model requires a traversal of the DAG so to select an appropriate cut. For each node we estimate the screen space error using a bounding sphere and the simplification error recorded during the construction stage.</p>

<p>The traversal of the tree prioritize the nodes with the highest error and each node is added to the cut if:</p>
<ul>
<li>its parents are in the cut
<li>the mesh associated with the node is in the GPU cache
<li>the error is above the selected treshold and our triangle budget is not spent already.
<li>the node saturated bounding sphere is in the view frustum
</ul>
<p>the traversal keeps going for a few nodes to allow the cache to be populated.</p>

<p>In order to simplify the traversal the error and the bounding sphere is 'saturated': we ensure that the error of a node is always bigger than the error of its children and the bounding sphere of a node contains the sphere of all its children. This ensures that no matter the view-point the screen space error of a node is always bigger than the screen space error of its children and in turn that a parent is always traversed before its children.</p>

<p>Once the cut has been selected for each node we collect the exiting edges in the DAG and for each one we render the corresponding patch using a single OpenGL call over the Vertex Buffer Object where the data is loaded. This results in a very low number of OpenGL function calls (on the order of one thousand) even for large models.</p>

<h3>Network Streaming</h3>
<p>Streaming over HTTP is implemented taking advantage of the HTTP Range header which allows to retrieve arbitrary sections of a file: the header and the index of the nodes at the beginning of the file specify the offsets of the nodes which are retrieved on demand.</p>
<p>This system require no server or special configurations of the HTTP server.</p>

<h3>WebGL rendering</h3>
<p>We built a WebGL implementation of the renderer (based on SpiderGL). Javascript performances were not a limit due to the minimal processing required (a simple traversal of a tree), and the limitations of OpenGL support in the browsers not a problem due to the very basic OpenGL features requested (Vertex Buffer Objects).</p>

<h2>Textures</h2>
<p>In principle the format can support texture coordinates and 1 texture per node. In practice the additional bytes per vertex and problems with parametrization of potentially very large and complex surfaces, makes this approach impractical.</p>
<p>It is more efficient to use a number of projective textures, especially in the common case where the colour information comes from a photographic campaign, since each photo is already a projective texture.</p>
<p>The format allows each patch to be further split into fragments according to a number of textures, effectively assigning each triangle to a texture.</p>

<p>Computation of the optimal set ot projective texture and patch subdivision is a very complicated problem, we provide a first implementation that obtains good results with a limited number of images.</p>

<p>The basic idea is to find the list of images projecting into each triangle, and rank them according to the sampling, pick the best and group all the triangles with the same image into the same patch fragment.</p>
<p>The problems with this naive approach lies in the potentially large number of fragments, high texture fragmentation or large unused texture space, and visible seams when switching from one texture to another.</p>
<p>As a first step we average all the textures backprojecting onto each other using the triangle matching information, then we apply a simple greedy approach: we start from a triangle and try to grow a patch of nearby triangles all assigned to the same image provided it passes a certain treshold. All texture fragmens from the triangles of the same node are packed together in a single texture (and the relative projection matrixes updated) to minimized context switching. (see picture)</p>

</body>
</html>
